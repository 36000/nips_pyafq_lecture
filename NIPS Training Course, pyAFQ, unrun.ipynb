{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a1973ed",
   "metadata": {},
   "source": [
    "# pyAFQ: Automated Fiber Quantification ... in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2631a596",
   "metadata": {},
   "source": [
    "## Section 1: Four benefits of using pyAFQ compared to matlab AFQ:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b0b126",
   "metadata": {},
   "source": [
    "### 1. Free and open source\n",
    "    1. No need for a MATLAB license"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a345b2a",
   "metadata": {},
   "source": [
    "### 2. Easier to run on cloud services\n",
    "    1. For example, Amazon Web Services (AWS) or Microsoft Azure\n",
    "    2. pyAFQ's only dependencies are other python packages which are installed automatically using \"Package Installer for Python (pip)\"\n",
    "![](install_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbd99d3",
   "metadata": {},
   "source": [
    "### 3. Easier to run on BIDS datasets\n",
    "1. The Brain Imaging Data Structure (BIDS) is a way of organizing neuroimaging and behavioral data\n",
    "2. BIDS is a community effort\n",
    "3. There are many [benefits to BIDS](https://bids.neuroimaging.io/benefits.html):\n",
    "    1. It will be easy for another researcher to work on your data\n",
    "        1. For example, by using BIDS you will save time trying to understand and reuse data acquired by a graduate student or postdoc that has already left the lab.\n",
    "        \n",
    "    2. There is a growing number of data analysis software that understand BIDS data, like pyAFQ\n",
    "    \n",
    "    3. Databases such as OpenNeuro.org, LORIS, COINS, XNAT, SciTran, and others will accept and export datasets organized according to BIDS.\n",
    "        1. If you ever plan to share your data publicly (some journals require this) you can speed up the curation process by using BIDS.\n",
    "        \n",
    "    4. There are validation tools (also available online) that can check your dataset integrity and let you easily spot missing values.\n",
    "    \n",
    "![BIDS diagram](bids_diagram.png)\n",
    "\n",
    "Image source: http://bids.neuroimaging.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cdb35c",
   "metadata": {},
   "source": [
    "### 4. Easier to customize methods and integrate with other pipelines\n",
    "    1. pyAFQ provides tools to import tractography or mori group files from the original Matlab based AFQ to pyAFQ\n",
    "    2. pyAFQ can use tractography from other pipelines like MRtrix3 or Qsiprep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f834933",
   "metadata": {},
   "source": [
    "## Section 2: Four pyAFQ code examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae5d084",
   "metadata": {},
   "source": [
    "### Tutorial 1: pyAFQ API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a517df1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "from IPython.display import IFrame\n",
    "\n",
    "from AFQ.api.group import GroupAFQ\n",
    "import AFQ.data.fetch as afd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ce1dc8",
   "metadata": {},
   "source": [
    "### Step 1: Organizing the data\n",
    "In these examples we use the first subject and first session from the High angular resolution diffusion imaging ([HARDI](https://purl.stanford.edu/ng782rw8378)) dataset from Stanford’s Vista Lab\n",
    "\n",
    "Anatomical data (anat) and Diffusion-weighted imaging data (dwi) are then extracted, formatted to be BIDS compliant, and placed in the AFQ data directory (by default in the users home directory) under:\n",
    "\n",
    "AFQ_data/stanford_hardi/\n",
    "\n",
    "This data represents the required preprocessed diffusion data necessary for intializing the GroupAFQ object. You will not need to run this function when you are using your own data. If your own data is already in BIDS, you do not need to perform this step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776f7bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "afd.organize_stanford_data(path=\"./my_example_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bd171a",
   "metadata": {},
   "source": [
    "### Step 2: Initialize a GroupAFQ object\n",
    "Creates a GroupAFQ object, that encapsulates tractometry. This object can be used to manage the entire AFQ pipeline, including:\n",
    "\n",
    "    Tractography\n",
    "\n",
    "    Registration\n",
    "\n",
    "    Segmentation\n",
    "\n",
    "    Cleaning\n",
    "\n",
    "    Profiling\n",
    "\n",
    "    Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a277ebd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "myafq = GroupAFQ(bids_path='./my_example_dataset/stanford_hardi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cf7a8a",
   "metadata": {},
   "source": [
    "### Step 3: Run pyAFQ\n",
    "After defining your pyAFQ API object, you can ask for the output of any step of the pipeline. It is common for users to just call export_all (for example, myafq.export_all()). However, if the user only wants the tractography, the user can instead call myafq.export(“streamlines”). In this case, we run myafq.export(\"all_bundles_figure\") to export an interactive visualization of the tractometry results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977139e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bundle_html = myafq.export(\"all_bundles_figure\")\n",
    "plotly.io.show(bundle_html[\"01\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a768fa",
   "metadata": {},
   "source": [
    "### Tutorial 2: Looking at the results of CSD vs DTI\n",
    "There are many choices of which methods to use when running a tracyometry pipeline. For example, to perform tractography, it is necessary to estimate a fiber orientation density function (fODF). pyAFQ makes it easy for users to choose which methods to use. Our default, used in Tutorial 1, is to use constrained spherical deconvolution (CSD). In this tutorial, we will use the diffusion tensor imaging (DTI) model for the fODF instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5647b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "myafq_dti = GroupAFQ(\n",
    "    bids_path='./my_example_dataset/stanford_hardi',\n",
    "    tracking_params={\"odf_model\": \"DTI\", \"directions\": \"det\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc5b764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# myafq_dti.export(\"indiv_bundles_figures\")\n",
    "\n",
    "IFrame(src=\"./my_example_dataset/stanford_hardi/derivatives/afq/sub-01/ses-01/viz_bundles/sub-01_ses-01_dwi_space-RASMM_model-DTI_desc-det-AFQ_ATR_L_viz.html\", width=700, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c29ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# myafq.export(\"indiv_bundles_figures\")\n",
    "\n",
    "IFrame(src=\"./my_example_dataset/stanford_hardi/derivatives/afq/sub-01/ses-01/viz_bundles/sub-01_ses-01_dwi_space-RASMM_model-CSD_desc-prob-AFQ_ATR_L_viz.html\", width=700, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6ee82f",
   "metadata": {},
   "source": [
    "### Tutorial 3: Plotting Custom Bundles\n",
    "pyAFQ is designed to be customizable. This example shows how you can customize it to define a new bundle based on both waypoint ROIs of your design, as well as endpoint ROIs of your design.\n",
    "\n",
    "In these example, we run pyAFQ with both the custom ROIs and the default waypoint ROIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf6832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from AFQ.api.group import GroupAFQ\n",
    "import AFQ.api.bundle_dict as abd\n",
    "import AFQ.data.fetch as afd\n",
    "from AFQ.definitions.image import RoiImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06801da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "afd.organize_stanford_data(path=\"./my_custom_bundles_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414c6a8a",
   "metadata": {},
   "source": [
    "Custom bundles are defined by passing a custom bundle_info dictionary to AFQ.api.bundle_dict.BundleDict: The keys of bundle_info are bundle names; the values are another dictionary describing the bundle, with these key-value pairs:\n",
    "\n",
    "- 'include' : a list of paths to Nifti files containing inclusion ROI(s).\n",
    "  One must either have at least 1 include ROI, or 'start' or 'end' ROIs.\n",
    "- 'exclude' : a list of paths to Nifti files containing exclusion ROI(s),\n",
    "  optional.\n",
    "- 'start' : path to a Nifti file containing the start ROI, optional\n",
    "- 'end' : path to a Nifti file containing the end ROI, optional\n",
    "- 'cross_midline' : boolean describing whether the bundle is required to\n",
    "  cross the midline (True) or prohibited from crossing (False), optional.\n",
    "  If None, the bundle may or may not cross the midline.\n",
    "- 'space' : a string which is either 'template' or 'subject', optional\n",
    "\n",
    "If this field is not given or 'template' is given, the ROI will be\n",
    "transformed from template to subject space before being used.\n",
    "- 'prob_map' : path to a Nifti file which is the probability map,\n",
    "  optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f962d1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "or_rois = afd.read_or_templates(as_img=False)\n",
    "print(or_rois[\"left_OR_1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f8884d",
   "metadata": {},
   "source": [
    "The cell above generates a dicitionary containing paths to Nifti files with our custom ROIs in them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5598425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bundles = abd.BundleDict({\n",
    "    \"L_OR\": {\n",
    "        \"include\": [\n",
    "            or_rois[\"left_OR_1\"],\n",
    "            or_rois[\"left_OR_2\"]],\n",
    "        \"exclude\": [\n",
    "            or_rois[\"left_OP_MNI\"],\n",
    "            or_rois[\"left_TP_MNI\"],\n",
    "            or_rois[\"left_pos_thal_MNI\"]],\n",
    "        \"start\": or_rois['left_thal_MNI'],\n",
    "        \"end\": or_rois['left_V1_MNI'],\n",
    "        \"cross_midline\": False,\n",
    "    },\n",
    "    \"R_OR\": {\n",
    "        \"include\": [\n",
    "            or_rois[\"right_OR_1\"],\n",
    "            or_rois[\"right_OR_2\"]],\n",
    "        \"exclude\": [\n",
    "            or_rois[\"right_OP_MNI\"],\n",
    "            or_rois[\"right_TP_MNI\"],\n",
    "            or_rois[\"right_pos_thal_MNI\"]],\n",
    "        \"start\": or_rois['right_thal_MNI'],\n",
    "        \"end\": or_rois['right_V1_MNI'],\n",
    "        \"cross_midline\": False\n",
    "    }\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c795eeb",
   "metadata": {},
   "source": [
    "It is easy to combine bundle dictionaries. Here, we add in the default bundles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb222ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bundles = bundles + abd.BundleDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4235cac",
   "metadata": {},
   "source": [
    "The bundle dictionary can be passed to the GroupAFQ object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403b5c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_afq_OR = GroupAFQ(\n",
    "    bids_path=\"./my_custom_bundles_dataset/stanford_hardi\",\n",
    "    bundle_info=bundles,\n",
    "    tracking_params={\"n_seeds\": 3, \"seed_mask\": RoiImage()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0872f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle_html = my_afq_OR.export(\"all_bundles_figure\")\n",
    "plotly.io.show(bundle_html[\"01\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a019654",
   "metadata": {},
   "source": [
    "### Tutorial 4: A different approach to bundle recognition, RecoBundles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a35a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "\n",
    "from AFQ.api.group import GroupAFQ\n",
    "import AFQ.data.fetch as afd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5528642a",
   "metadata": {},
   "outputs": [],
   "source": [
    "afd.organize_stanford_data(path=\"./my_recobundles_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9234911c",
   "metadata": {},
   "source": [
    "RecoBundles a different way to recognize white matter bundles than AFQ.\n",
    "RecoBundles uses bundle models as shape priors for detecting similar streamlines and bundles from the tractography, instead of AFQ which relies on regions of interest to recognize white matter bundles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d123c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "myafq_reco = GroupAFQ(\n",
    "    \"./my_recobundles_dataset/stanford_hardi\",\n",
    "    segmentation_params={\"seg_algo\": \"reco80\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a0347a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle_html = myafq_reco.export(\"all_bundles_figure\")\n",
    "plotly.io.show(bundle_html[\"01\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091db304",
   "metadata": {},
   "source": [
    "## Section 3: Five other features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0818ca6c",
   "metadata": {},
   "source": [
    "### 1. We developed a python library called cloudknot (https://nrdg.github.io/cloudknot/) to run python code on amazon web services"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3ae2ff",
   "metadata": {},
   "source": [
    "### 2. Can be used as a reconstruction pipeline of \"QSIprep\"\n",
    "        1. QSIprep is also free, open source, and uses BIDS.\n",
    "        2. Includes automatically generated preprocessing pipelines that correctly group, distortion correct, motion correct, denoise, coregister and resample your scans, producing visual reports and QC metrics.\n",
    "![](QSIprep.png)\n",
    "Image source: https://qsiprep.readthedocs.io/en/latest/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95345416",
   "metadata": {},
   "source": [
    "### 3. Can add custom tissue properties\n",
    "    As an example, one might have “ICVF” and “ODI” maps in a BIDS pipeline named “noddi”:\n",
    "\n",
    "        ~/my_bids_path/\n",
    "        ├── dataset_description.json\n",
    "        └── derivatives\n",
    "            ├── noddi\n",
    "            │   ├── dataset_description.json\n",
    "            │   └── sub-01\n",
    "            │       └── ses-01\n",
    "            │           └── sub-01_ses-01_ICVF.nii.gz\n",
    "            │           └── sub-01_ses-01_ODI.nii.gz\n",
    "            └── vistasoft\n",
    "                ├── dataset_description.json\n",
    "                └── sub-01\n",
    "                    └── ses-01\n",
    "                        └── dwi\n",
    "                            ├── sub-01_ses-01_dwi.bval\n",
    "                            ├── sub-01_ses-01_dwi.bvec\n",
    "                            └── sub-01_ses-01_dwi.nii.gz\n",
    "\n",
    "    You can use AFQ.definitions.image.ImageFile to provide these custom scalars to the AFQ.api objects:\n",
    "\n",
    "        ICVF_scalar = ImageFile(\n",
    "          suffix=\"ICVF\",\n",
    "          filters={\"scope\": \"noddi\"})\n",
    "\n",
    "        ODI_scalar = ImageFile(\n",
    "          suffix=\"ODI\",\n",
    "          filters={\"scope\": \"noddi\"})\n",
    "\n",
    "        api.GroupAFQ(\n",
    "          \"my_bids_path\",\n",
    "          scalars=[\"dti_fa\", \"dti_md\", ICVF_scalar, ODI_scalar])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ad2b83",
   "metadata": {},
   "source": [
    "### 4. Outputs can be used as inputs to AFQ-Insight or AFQ-Browser\n",
    "1. [AFQ-Insight](https://github.com/richford/AFQ-Insight) is an open source, python based statistical learning library for tractometry\n",
    "        \n",
    "2. [AFQ-Browser](https://yeatmanlab.github.io/AFQ-Browser/) is an open source, python based visualization library for tractometry\n",
    "    \n",
    "3. Outputs of Matlab based AFQ will also work for both of these"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a833619",
   "metadata": {},
   "source": [
    "### 5. Using the pyAFQ configuration file\n",
    "pyAFQ provides a [command-line interface (CLI)](https://yeatmanlab.github.io/pyAFQ/usage/index.html). After installing the software, and organizing the data, run:\n",
    "\n",
    "    pyAFQ /path/to/config.toml\n",
    "\n",
    "At minimum, the file should contain the BIDS path:\n",
    "\n",
    "    [BIDS_PARAMS]\n",
    "    bids_path = \"path/to/study\n",
    "\n",
    "But additional configuration options can be provided. See an example configuration file below:\n",
    "\n",
    "    [BIDS_PARAMS]\n",
    "    bids_path = \"path/to/study\n",
    "    preproc_pipeline = \"qsiprep\"\n",
    "    bids_filters = \"{'desc': 'preproc'}\"\n",
    "\n",
    "    [MAPPING]\n",
    "    mapping_definition = \"ItkMap(warp_suffix='xfm', warp_filters={'from': 'MNI152NLin2009cAsym', 'to': 'T1w', 'scope': 'qsiprep'})\"\n",
    "\n",
    "    [DATA]\n",
    "    brain_mask_definition = \"ImageFile(suffix='mask', filters={'desc': 'brain', 'space': 'T1w', 'scope': 'qsiprep'})\"\n",
    "    bundle_info = \"['ATR_L', 'ATR_R', 'CGC_L', 'CGC_R', 'CST_L', 'CST_R', 'IFO_L', 'IFO_R', 'ILF_L', 'ILF_R', 'SLF_L', 'SLF_R', 'ARC_L', 'ARC_R', 'UNC_L', 'UNC_R', 'AntFrontal', 'Motor', 'Occipital', 'Orbital', 'PostParietal', 'SupFrontal', 'SupParietal', 'Temporal']\"\n",
    "\n",
    "    [SEGMENTATION]\n",
    "    scalars = \"['dki_fa', 'dki_md', 'dki_awf', 'dki_mk']\"\n",
    "\n",
    "    [TRACTOGRAPHY_PARAMS]\n",
    "    seed_mask = \"ScalarImage('dki_fa')\"\n",
    "    stop_mask = \"ScalarImage('dki_fa')\"\n",
    "    odf_model = \"CSD\"\n",
    "    directions = \"prob\"\n",
    "\n",
    "    [SEGMENTATION_PARAMS]\n",
    "    parallel_segmentation = \"{'n_jobs': 1, 'engine': 'joblib', 'backend': 'loky'}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334881f8",
   "metadata": {},
   "source": [
    "## Section 4: Summary\n",
    "    1. pyAFQ is free and open source\n",
    "\n",
    "    2. pyAFQ is easy to install and run on the cloud\n",
    "\n",
    "    3. pyAFQ uses BIDS datasets\n",
    "\n",
    "    4. pyAFQ is highly customizable and integrates with other pipelines\n",
    "  \n",
    "Documentation: https://yeatmanlab.github.io/pyAFQ/ <br />\n",
    "Code: https://github.com/yeatmanlab/pyAFQ <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db85fe0",
   "metadata": {},
   "source": [
    "# Thank you for listening!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1cbb6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
